{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Train_Your_Own_Q&A_Models.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dkyos/data-analysis/blob/master/Train_Your_Own_Q%26A_Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bLOkc8ulPyB",
        "colab_type": "text"
      },
      "source": [
        "![DoctorBert](https://snag.gy/oF84Gk.jpg)\n",
        "\n",
        "\n",
        "![DoctorBert](https://i.ytimg.com/vi/nPemP-Q0Xn8/hqdefault.jpg)\n",
        "\n",
        "This is a Colab Demo of our DocProduct Tensorflow 2.0 Hackathon Project\n",
        "\n",
        "Project details can be seen on our Github repo\n",
        "https://github.com/Santosh-Gupta/DocProduct\n",
        "\n",
        "and our Devpost page\n",
        "https://devpost.com/software/nlp-doctor\n",
        "\n",
        "This notebook shows how you can use our code to train your own Q&A model but inserting your own data. \n",
        "\n",
        "The model architecture that is being trained is shown below\n",
        "\n",
        "![DoctorBert](https://i.imgur.com/IRCyKIL.jpg?1)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ZAC6FANdmou",
        "colab_type": "code",
        "cellView": "both",
        "outputId": "5114da2f-3f4c-448d-b99f-1f0528618668",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3063
        }
      },
      "source": [
        "#@title Install Faiss, TF 2.0, and our Github. Double Click to see code\n",
        "\n",
        "#To use CPU FAISS use\n",
        "!wget  https://anaconda.org/pytorch/faiss-cpu/1.2.1/download/linux-64/faiss-cpu-1.2.1-py36_cuda9.0.176_1.tar.bz2\n",
        "#To use GPU FAISS use\n",
        "# !wget  https://anaconda.org/pytorch/faiss-gpu/1.2.1/download/linux-64/faiss-gpu-1.2.1-py36_cuda9.0.176_1.tar.bz2\n",
        "!tar xvjf faiss-cpu-1.2.1-py36_cuda9.0.176_1.tar.bz2\n",
        "!cp -r lib/python3.6/site-packages/* /usr/local/lib/python3.6/dist-packages/\n",
        "!pip install mkl\n",
        "\n",
        "!pip install tensorflow-gpu==2.0.0-alpha0\n",
        "import tensorflow as tf\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "import tensorflow.keras.backend as K\n",
        "!pip install https://github.com/Santosh-Gupta/DocProduct/archive/master.zip\n",
        "    \n",
        "import argparse\n",
        "import os\n",
        "import requests\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-05-10 03:36:55--  https://anaconda.org/pytorch/faiss-cpu/1.2.1/download/linux-64/faiss-cpu-1.2.1-py36_cuda9.0.176_1.tar.bz2\n",
            "Resolving anaconda.org (anaconda.org)... 104.17.93.24, 104.17.92.24, 2606:4700::6811:5d18, ...\n",
            "Connecting to anaconda.org (anaconda.org)|104.17.93.24|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 FOUND\n",
            "Location: https://binstar-cio-packages-prod.s3.amazonaws.com/5a15c9c5c376961204909d87/5aa7f0a65571b411e5c259be?response-content-disposition=attachment%3B%20filename%3D%22faiss-cpu-1.2.1-py36_cuda9.0.176_1.tar.bz2%22%3B%20filename%2A%3DUTF-8%27%27faiss-cpu-1.2.1-py36_cuda9.0.176_1.tar.bz2&response-content-type=application%2Fx-tar&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Expires=60&X-Amz-Date=20190510T033656Z&X-Amz-SignedHeaders=host&X-Amz-Security-Token=AgoJb3JpZ2luX2VjEHIaCXVzLWVhc3QtMSJGMEQCIECtnkIy%2FeXVbJUBVmYMIPLpNNxgA5hw0dsf%2BGlRs4%2B8AiAtXT3G5jBUofVTxShzKuwumeyaWWkHKlfHiK7MhQ2BJSraAwhrEAAaDDQ1NTg2NDA5ODM3OCIMLJKTDy8nMta%2F%2F1ozKrcDBedHKyJ%2BHoGkiefU2GuVkj6PWqlYqVrh5xb0kKFj8DUCeOYJH%2B3Ua%2F104Opa%2BrKjTF5NNyT0npD%2F5Ifidu%2BOxMLZ4nrCn%2B59cHf5DDj%2F71Un6oQ9%2BoOWwK%2FzZlYnTCv1jZaDQV2K0v6ij5ONIQh6hW6geTsa0mXfyaEKcLmJt1tXlbdg5oJSq9O4o5SSeURjbgfJLUjmW9jeXz5NgNoNSFuwDgEHSy9iIJp3hop6bLp5djItUQYw3TEKFMzyLG851wI0QqZVOJEFgAJ4DzAXv8Gq%2FoLDmAGSLFb62XL9TxxF%2Bj5DThuoaEnASTVemYduGNOJ2hhOwxhdxhfJ0ImZIjHjpejKV3Q1%2BV5AqxK12YA9qtB6FMtMzboLRBGt1dWnOU9cNO0JnQ1bSy9n40yMimm01s7dR4O%2F%2F7odAkIaLrVBKCUYZtlgAiaMF8p6NabEPvMK4NuVoozm1Uv%2FOAPwbgqGxk%2FfPech8Y%2BtRYT%2FMiHi1uLMF3fDHvbSjTQ09POeB8XaKGydCzomWutD%2B19WxNaCTwMbVhObFMmeED3W2DiqTrQCOh4YvYKXb%2FJf5gvC%2BvddJNUM7DDirdPmBTq1Acewt0hAQOJgoONwl9AWqwqMKCfXerodjWZzDvThv5ucA57Qb6321FJP6rpSfgxYQX%2F1d1WyV0GCkLarAoPkAiF5utZoXlSlw2dNuTnq66nQ%2F5%2FRqL0M5ty1YIYWNkSm3pZtLf3Y8td2Hg09UaQ4YwGd3OxCzIGZNj1hckJ4aNMMu1CKoA08MirabJuspfrOIGm1v7N%2BPkcXOM9gXARnjtRW4qQi6jofqSQ28Fpuk81rRh%2BhIrk%3D&X-Amz-Credential=ASIAWUI46DZFBATOEYGK%2F20190510%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=379f8fb9df5b7a7b2dd0745c2a26aae6385d4235c58eca987f24e17cc659dde8 [following]\n",
            "--2019-05-10 03:36:56--  https://binstar-cio-packages-prod.s3.amazonaws.com/5a15c9c5c376961204909d87/5aa7f0a65571b411e5c259be?response-content-disposition=attachment%3B%20filename%3D%22faiss-cpu-1.2.1-py36_cuda9.0.176_1.tar.bz2%22%3B%20filename%2A%3DUTF-8%27%27faiss-cpu-1.2.1-py36_cuda9.0.176_1.tar.bz2&response-content-type=application%2Fx-tar&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Expires=60&X-Amz-Date=20190510T033656Z&X-Amz-SignedHeaders=host&X-Amz-Security-Token=AgoJb3JpZ2luX2VjEHIaCXVzLWVhc3QtMSJGMEQCIECtnkIy%2FeXVbJUBVmYMIPLpNNxgA5hw0dsf%2BGlRs4%2B8AiAtXT3G5jBUofVTxShzKuwumeyaWWkHKlfHiK7MhQ2BJSraAwhrEAAaDDQ1NTg2NDA5ODM3OCIMLJKTDy8nMta%2F%2F1ozKrcDBedHKyJ%2BHoGkiefU2GuVkj6PWqlYqVrh5xb0kKFj8DUCeOYJH%2B3Ua%2F104Opa%2BrKjTF5NNyT0npD%2F5Ifidu%2BOxMLZ4nrCn%2B59cHf5DDj%2F71Un6oQ9%2BoOWwK%2FzZlYnTCv1jZaDQV2K0v6ij5ONIQh6hW6geTsa0mXfyaEKcLmJt1tXlbdg5oJSq9O4o5SSeURjbgfJLUjmW9jeXz5NgNoNSFuwDgEHSy9iIJp3hop6bLp5djItUQYw3TEKFMzyLG851wI0QqZVOJEFgAJ4DzAXv8Gq%2FoLDmAGSLFb62XL9TxxF%2Bj5DThuoaEnASTVemYduGNOJ2hhOwxhdxhfJ0ImZIjHjpejKV3Q1%2BV5AqxK12YA9qtB6FMtMzboLRBGt1dWnOU9cNO0JnQ1bSy9n40yMimm01s7dR4O%2F%2F7odAkIaLrVBKCUYZtlgAiaMF8p6NabEPvMK4NuVoozm1Uv%2FOAPwbgqGxk%2FfPech8Y%2BtRYT%2FMiHi1uLMF3fDHvbSjTQ09POeB8XaKGydCzomWutD%2B19WxNaCTwMbVhObFMmeED3W2DiqTrQCOh4YvYKXb%2FJf5gvC%2BvddJNUM7DDirdPmBTq1Acewt0hAQOJgoONwl9AWqwqMKCfXerodjWZzDvThv5ucA57Qb6321FJP6rpSfgxYQX%2F1d1WyV0GCkLarAoPkAiF5utZoXlSlw2dNuTnq66nQ%2F5%2FRqL0M5ty1YIYWNkSm3pZtLf3Y8td2Hg09UaQ4YwGd3OxCzIGZNj1hckJ4aNMMu1CKoA08MirabJuspfrOIGm1v7N%2BPkcXOM9gXARnjtRW4qQi6jofqSQ28Fpuk81rRh%2BhIrk%3D&X-Amz-Credential=ASIAWUI46DZFBATOEYGK%2F20190510%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=379f8fb9df5b7a7b2dd0745c2a26aae6385d4235c58eca987f24e17cc659dde8\n",
            "Resolving binstar-cio-packages-prod.s3.amazonaws.com (binstar-cio-packages-prod.s3.amazonaws.com)... 52.216.176.243\n",
            "Connecting to binstar-cio-packages-prod.s3.amazonaws.com (binstar-cio-packages-prod.s3.amazonaws.com)|52.216.176.243|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4106816 (3.9M) [application/x-tar]\n",
            "Saving to: ‘faiss-cpu-1.2.1-py36_cuda9.0.176_1.tar.bz2’\n",
            "\n",
            "faiss-cpu-1.2.1-py3 100%[===================>]   3.92M  18.6MB/s    in 0.2s    \n",
            "\n",
            "2019-05-10 03:36:56 (18.6 MB/s) - ‘faiss-cpu-1.2.1-py36_cuda9.0.176_1.tar.bz2’ saved [4106816/4106816]\n",
            "\n",
            "info/hash_input.json\n",
            "info/has_prefix\n",
            "info/index.json\n",
            "info/git\n",
            "info/files\n",
            "info/LICENSE.txt\n",
            "info/about.json\n",
            "info/paths.json\n",
            "lib/python3.6/site-packages/faiss-0.1-py3.6.egg-info/dependency_links.txt\n",
            "lib/python3.6/site-packages/faiss-0.1-py3.6.egg-info/not-zip-safe\n",
            "lib/python3.6/site-packages/faiss-0.1-py3.6.egg-info/requires.txt\n",
            "lib/python3.6/site-packages/faiss-0.1-py3.6.egg-info/top_level.txt\n",
            "lib/python3.6/site-packages/faiss-0.1-py3.6.egg-info/native_libs.txt\n",
            "info/test/run_test.py\n",
            "info/test/run_test.sh\n",
            "info/test/tests/run_tests.sh\n",
            "lib/python3.6/site-packages/faiss-0.1-py3.6.egg-info/SOURCES.txt\n",
            "info/recipe/conda_build_config.yaml\n",
            "info/recipe/build.sh\n",
            "info/test/tests/CMakeLists.txt\n",
            "info/test/tests/Makefile\n",
            "info/recipe/meta.yaml.template\n",
            "lib/python3.6/site-packages/faiss-0.1-py3.6.egg-info/PKG-INFO\n",
            "info/test/tests/test_factory.py\n",
            "info/test/tests/test_ivfpq_codec.cpp\n",
            "info/recipe/meta.yaml\n",
            "info/recipe/setup.py\n",
            "info/test/tests/test_blas.cpp\n",
            "info/recipe/makefile.inc\n",
            "info/test/tests/test_ivfpq_indexing.cpp\n",
            "info/test/tests/test_ondisk_ivf.cpp\n",
            "info/test/tests/test_build_blocks.py\n",
            "info/test/tests/test_merge.cpp\n",
            "info/test/tests/test_pairs_decoding.cpp\n",
            "info/test/tests/test_index_composite.py\n",
            "lib/python3.6/site-packages/faiss/__init__.py\n",
            "lib/python3.6/site-packages/faiss/__pycache__/__init__.cpython-36.pyc\n",
            "info/test/tests/test_index.py\n",
            "info/test/tests/test_blas\n",
            "lib/python3.6/site-packages/faiss/__pycache__/swigfaiss.cpython-36.pyc\n",
            "lib/python3.6/site-packages/faiss/swigfaiss.py\n",
            "lib/python3.6/site-packages/faiss/_swigfaiss.so\n",
            "Requirement already satisfied: mkl in /usr/local/lib/python3.6/dist-packages (2019.0)\n",
            "Requirement already satisfied: intel-openmp in /usr/local/lib/python3.6/dist-packages (from mkl) (2019.0)\n",
            "Collecting tensorflow-gpu==2.0.0-alpha0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1a/66/32cffad095253219d53f6b6c2a436637bbe45ac4e7be0244557210dc3918/tensorflow_gpu-2.0.0a0-cp36-cp36m-manylinux1_x86_64.whl (332.1MB)\n",
            "\u001b[K     |████████████████████████████████| 332.1MB 66kB/s \n",
            "\u001b[?25hRequirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (0.7.1)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (0.2.2)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (0.33.1)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (3.7.1)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (0.7.1)\n",
            "Collecting tb-nightly<1.14.0a20190302,>=1.14.0a20190301 (from tensorflow-gpu==2.0.0-alpha0)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a9/51/aa1d756644bf4624c03844115e4ac4058eff77acd786b26315f051a4b195/tb_nightly-1.14.0a20190301-py3-none-any.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 42.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.0.7)\n",
            "Collecting tf-estimator-nightly<1.14.0.dev2019030116,>=1.14.0.dev2019030115 (from tensorflow-gpu==2.0.0-alpha0)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/13/82/f16063b4eed210dc2ab057930ac1da4fbe1e91b7b051a6c8370b401e6ae7/tf_estimator_nightly-1.14.0.dev2019030115-py2.py3-none-any.whl (411kB)\n",
            "\u001b[K     |████████████████████████████████| 419kB 54.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.0.9)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.15.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.1.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.12.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.16.3)\n",
            "Collecting google-pasta>=0.1.2 (from tensorflow-gpu==2.0.0-alpha0)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f9/68/a14620bfb042691f532dcde8576ff82ee82e4c003cdc0a3dbee5f289cee6/google_pasta-0.1.6-py3-none-any.whl (51kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 32.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-gpu==2.0.0-alpha0) (41.0.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow-gpu==2.0.0-alpha0) (0.15.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow-gpu==2.0.0-alpha0) (3.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu==2.0.0-alpha0) (2.8.0)\n",
            "Installing collected packages: tb-nightly, tf-estimator-nightly, google-pasta, tensorflow-gpu\n",
            "Successfully installed google-pasta-0.1.6 tb-nightly-1.14.0a20190301 tensorflow-gpu-2.0.0a0 tf-estimator-nightly-1.14.0.dev2019030115\n",
            "Collecting https://github.com/Santosh-Gupta/DocProduct/archive/master.zip\n",
            "\u001b[?25l  Downloading https://github.com/Santosh-Gupta/DocProduct/archive/master.zip\n",
            "\u001b[K     \\ 23.3MB 202kB/s\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from MedicalQA==0.1.0) (1.16.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from MedicalQA==0.1.0) (3.0.3)\n",
            "Collecting tensorflow==2.0.0-alpha0 (from MedicalQA==0.1.0)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/29/39/f99185d39131b8333afcfe1dcdb0629c2ffc4ecfb0e4c14ca210d620e56c/tensorflow-2.0.0a0-cp36-cp36m-manylinux1_x86_64.whl (79.9MB)\n",
            "\u001b[K     |████████████████████████████████| 79.9MB 543kB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow-gpu==2.0.0-alpha0 in /usr/local/lib/python3.6/dist-packages (from MedicalQA==0.1.0) (2.0.0a0)\n",
            "Requirement already satisfied: Keras in /usr/local/lib/python3.6/dist-packages (from MedicalQA==0.1.0) (2.2.4)\n",
            "Collecting keras-pos-embd==0.9.0 (from MedicalQA==0.1.0)\n",
            "  Downloading https://files.pythonhosted.org/packages/56/5e/7b1e933104a25f2039b6788e392a650671e3bcbee6404ea29dcb92295614/keras-pos-embd-0.9.0.tar.gz\n",
            "Collecting keras-transformer==0.21.0 (from MedicalQA==0.1.0)\n",
            "  Downloading https://files.pythonhosted.org/packages/d3/3a/ad25f5c71adc6b8aa73f71b1367be873b4103125a614ba57c006d1a9b1ff/keras-transformer-0.21.0.tar.gz\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from MedicalQA==0.1.0) (4.28.1)\n",
            "Requirement already satisfied: faiss in /usr/local/lib/python3.6/dist-packages (from MedicalQA==0.1.0) (0.1)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.6/dist-packages (from MedicalQA==0.1.0) (0.0)\n",
            "Collecting pycurl (from MedicalQA==0.1.0)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e8/e4/0dbb8735407189f00b33d84122b9be52c790c7c3b25286826f4e1bdb7bde/pycurl-7.43.0.2.tar.gz (214kB)\n",
            "\u001b[K     |████████████████████████████████| 215kB 51.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from MedicalQA==0.1.0) (1.12.0)\n",
            "Collecting argparse (from MedicalQA==0.1.0)\n",
            "  Downloading https://files.pythonhosted.org/packages/f2/94/3af39d34be01a24a6e65433d19e107099374224905f1e0cc6bbe1fd22a2f/argparse-1.4.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->MedicalQA==0.1.0) (2.5.3)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->MedicalQA==0.1.0) (2.4.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->MedicalQA==0.1.0) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->MedicalQA==0.1.0) (1.1.0)\n",
            "Requirement already satisfied: tf-estimator-nightly<1.14.0.dev2019030116,>=1.14.0.dev2019030115 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-alpha0->MedicalQA==0.1.0) (1.14.0.dev2019030115)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-alpha0->MedicalQA==0.1.0) (0.33.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-alpha0->MedicalQA==0.1.0) (1.1.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-alpha0->MedicalQA==0.1.0) (3.7.1)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-alpha0->MedicalQA==0.1.0) (0.2.2)\n",
            "Requirement already satisfied: tb-nightly<1.14.0a20190302,>=1.14.0a20190301 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-alpha0->MedicalQA==0.1.0) (1.14.0a20190301)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-alpha0->MedicalQA==0.1.0) (1.0.7)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-alpha0->MedicalQA==0.1.0) (1.0.9)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-alpha0->MedicalQA==0.1.0) (0.7.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-alpha0->MedicalQA==0.1.0) (1.15.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-alpha0->MedicalQA==0.1.0) (0.1.6)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-alpha0->MedicalQA==0.1.0) (0.7.1)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras->MedicalQA==0.1.0) (1.2.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras->MedicalQA==0.1.0) (3.13)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras->MedicalQA==0.1.0) (2.8.0)\n",
            "Collecting keras-multi-head==0.18.0 (from keras-transformer==0.21.0->MedicalQA==0.1.0)\n",
            "  Downloading https://files.pythonhosted.org/packages/d7/5d/8156def9ca75c55bb87819618e9a3e1f8e587c722570e2e93ad616b9269d/keras-multi-head-0.18.0.tar.gz\n",
            "Collecting keras-layer-normalization==0.11.0 (from keras-transformer==0.21.0->MedicalQA==0.1.0)\n",
            "  Downloading https://files.pythonhosted.org/packages/c6/b0/c786d5a5e79d985281a06da0a1f3f559cf425921464e6b07b9f1cb093a5a/keras-layer-normalization-0.11.0.tar.gz\n",
            "Collecting keras-position-wise-feed-forward==0.4.0 (from keras-transformer==0.21.0->MedicalQA==0.1.0)\n",
            "  Downloading https://files.pythonhosted.org/packages/91/21/4eefba0b6ea01de9c6e469970a39dbdbce14e5183a20274d9a181f55eaa8/keras-position-wise-feed-forward-0.4.0.tar.gz\n",
            "Collecting keras-embed-sim==0.3.0 (from keras-transformer==0.21.0->MedicalQA==0.1.0)\n",
            "  Downloading https://files.pythonhosted.org/packages/8e/16/b05954f9578ded225fd1bd56154ade949782c03b668a1fc424d5050e868a/keras-embed-sim-0.3.0.tar.gz\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sklearn->MedicalQA==0.1.0) (0.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib->MedicalQA==0.1.0) (41.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow==2.0.0-alpha0->MedicalQA==0.1.0) (3.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow==2.0.0-alpha0->MedicalQA==0.1.0) (0.15.2)\n",
            "Collecting keras-self-attention==0.39.0 (from keras-multi-head==0.18.0->keras-transformer==0.21.0->MedicalQA==0.1.0)\n",
            "  Downloading https://files.pythonhosted.org/packages/91/70/51150779d5bbd1488a30c62026b141073873faf81eac7a62c6460cb5efe0/keras-self-attention-0.39.0.tar.gz\n",
            "Building wheels for collected packages: MedicalQA, keras-pos-embd, keras-transformer, pycurl, keras-multi-head, keras-layer-normalization, keras-position-wise-feed-forward, keras-embed-sim, keras-self-attention\n",
            "  Building wheel for MedicalQA (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-5bvxjfze/wheels/bf/6a/38/37bcbad470418d91e9d79c4bc452f69fd2f93b996deb347539\n",
            "  Building wheel for keras-pos-embd (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/fb/97/65/170068ed0a4bd2185d561afee6c93e23e87e8d735d61389590\n",
            "  Building wheel for keras-transformer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/19/9f/ff/3b38f44f6db035cfd33cff4909edcc4864a6aeec80d9deaf23\n",
            "  Building wheel for pycurl (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/d2/85/ae/ebf5ff0f1368869d082b4863df492bf54c661bf6306a2bdfde\n",
            "  Building wheel for keras-multi-head (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/88/83/d7680876b48974c3c11fc334ed1d0a480ae218764062385bf3\n",
            "  Building wheel for keras-layer-normalization (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/86/dc/2e/3ac54a6b948bff68cb999d210c6ebf9e22df7a4a24cf114436\n",
            "  Building wheel for keras-position-wise-feed-forward (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/a1/13/3c913efde102d56ac584f61004a9fec6f8859b6feec6aa7aa7\n",
            "  Building wheel for keras-embed-sim (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/bf/f2/c6/0610efe9730c708b24ec29c25cebd38eb485acbc2eee7b5634\n",
            "  Building wheel for keras-self-attention (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/6c/d6/3e/cac34bf035198e38947006910f3ecb25613d6d9d76ea6d8ef2\n",
            "Successfully built MedicalQA keras-pos-embd keras-transformer pycurl keras-multi-head keras-layer-normalization keras-position-wise-feed-forward keras-embed-sim keras-self-attention\n",
            "Installing collected packages: tensorflow, keras-pos-embd, keras-self-attention, keras-multi-head, keras-layer-normalization, keras-position-wise-feed-forward, keras-embed-sim, keras-transformer, pycurl, argparse, MedicalQA\n",
            "  Found existing installation: tensorflow 1.13.1\n",
            "    Uninstalling tensorflow-1.13.1:\n",
            "      Successfully uninstalled tensorflow-1.13.1\n",
            "Successfully installed MedicalQA-0.1.0 argparse-1.4.0 keras-embed-sim-0.3.0 keras-layer-normalization-0.11.0 keras-multi-head-0.18.0 keras-pos-embd-0.9.0 keras-position-wise-feed-forward-0.4.0 keras-self-attention-0.39.0 keras-transformer-0.21.0 pycurl-7.43.0.2 tensorflow-2.0.0a0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "argparse",
                  "tensorflow"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rROvFnv3Psa_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download pre-trained Bert. This version downloads the bioBert pretrained model but you can set it to download any Bert pretrained model. \n",
        "\n",
        "import urllib.request\n",
        "\n",
        "urllib.request.urlretrieve('https://github.com/naver/biobert-pretrained/releases/download/v1.0-pubmed-pmc/biobert_pubmed_pmc.tar.gz', 'BioBert.tar.gz') #Link to pre-trained BERT\n",
        "\n",
        "if not os.path.exists('BioBertFolder'):\n",
        "    os.makedirs('BioBertFolder')\n",
        "    \n",
        "import tarfile\n",
        "tar = tarfile.open(\"BioBert.tar.gz\")\n",
        "tar.extractall(path='BioBertFolder/')\n",
        "tar.close()\n",
        "\n",
        "os.listdir('BioBertFolder/pubmed_pmc_470k')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3l9_y5pedzkD",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Download our fine-tuned model (use is optional). . Double Click to see code\n",
        "\n",
        "import urllib.request\n",
        "\n",
        "urllib.request.urlretrieve('https://github.com/naver/biobert-pretrained/releases/download/v1.0-pubmed-pmc/biobert_pubmed_pmc.tar.gz', 'BioBert.tar.gz')\n",
        "\n",
        "if not os.path.exists('BioBertFolder'):\n",
        "    os.makedirs('BioBertFolder')\n",
        "    \n",
        "import tarfile\n",
        "tar = tarfile.open(\"BioBert.tar.gz\")\n",
        "tar.extractall(path='BioBertFolder/')\n",
        "tar.close()\n",
        "\n",
        "os.listdir('BioBertFolder/pubmed_pmc_470k')\n",
        "\n",
        "def download_file_from_google_drive(id, destination):\n",
        "    URL = \"https://docs.google.com/uc?export=download\"\n",
        "\n",
        "    session = requests.Session()\n",
        "\n",
        "    response = session.get(URL, params = { 'id' : id }, stream = True)\n",
        "    token = get_confirm_token(response)\n",
        "\n",
        "    if token:\n",
        "        params = { 'id' : id, 'confirm' : token }\n",
        "        response = session.get(URL, params = params, stream = True)\n",
        "\n",
        "    save_response_content(response, destination)    \n",
        "\n",
        "def get_confirm_token(response):\n",
        "    for key, value in response.cookies.items():\n",
        "        if key.startswith('download_warning'):\n",
        "            return value\n",
        "\n",
        "    return None\n",
        "\n",
        "def save_response_content(response, destination):\n",
        "    CHUNK_SIZE = 32768\n",
        "\n",
        "    with open(destination, \"wb\") as f:\n",
        "        for chunk in response.iter_content(CHUNK_SIZE):\n",
        "            if chunk: # filter out keep-alive new chunks\n",
        "                f.write(chunk)\n",
        "\n",
        "if not os.path.exists('models'):\n",
        "    os.mkdir('models')\n",
        "    \n",
        "if not os.path.exists('models/bertffn_crossentropy'):\n",
        "    os.mkdir('models/bertffn_crossentropy')\n",
        "    \n",
        "if not os.path.exists('models/bertffn_crossentropy/bertffn'):\n",
        "    os.mkdir('models/bertffn_crossentropy/bertffn')\n",
        "    \n",
        "import requests\n",
        "\n",
        "if not os.path.exists('qa_embeddings'):\n",
        "    os.mkdir('qa_embeddings')\n",
        "    \n",
        "if not os.path.exists('chkpoint'):\n",
        "    os.mkdir('chkpoint')\n",
        "    \n",
        "file_id = '1thX75_cMkly5btgxTydgV6YDKnrUczZs'\n",
        "\n",
        "download_file_from_google_drive(file_id, 'models/bertffn_crossentropy/bertffn/bertffn.data-00000-of-00001')\n",
        "\n",
        "file_id = '11q29T38EysVPueD1PMQzZoKs5QFr3jRT'\n",
        "\n",
        "download_file_from_google_drive(file_id, 'models/bertffn_crossentropy/bertffn/bertffn.index')\n",
        "\n",
        "file_id = '1iI5Aow_7pQSmvpxGnMVKw9OJV-nckkxz'\n",
        "\n",
        "download_file_from_google_drive(file_id, 'models/bertffn_crossentropy/bertffn/bertffn.checkpoint')\n",
        "\n",
        "os.listdir('models/bertffn_crossentropy/bertffn')\n",
        "\n",
        "if not os.path.exists('FFNNFolder'):\n",
        "    os.makedirs('FFNNFolder')\n",
        "    \n",
        "file_id = '1-258Wlpp5UwwqCPezy6DRYBozO4wBUBw'\n",
        "\n",
        "download_file_from_google_drive(file_id, 'FFNNFolder/data-00000-of-00001')\n",
        "\n",
        "file_id = '1-9JmtJJ_XGV0wClZxs2Px4hDL-Pi9YA_'\n",
        "\n",
        "download_file_from_google_drive(file_id, 'FFNNFolder/index')\n",
        "\n",
        "file_id = '1-5hczUjQfCTpBg1HhpLrgXFTkve1xuRM'\n",
        "\n",
        "download_file_from_google_drive(file_id, 'FFNNFolder/checkpoint')\n",
        "\n",
        "#download data\n",
        "\n",
        "# file_id = '1blkZdV1BNWesD0mJ6Pm1H1IhCMYThkWJ'\n",
        "\n",
        "# download_file_from_google_drive(file_id, 'Float16Embeddings.pkl')\n",
        "\n",
        "if not os.path.exists('data'):\n",
        "    os.mkdir('data')\n",
        "    \n",
        "file_id = '1G-NhMv_AQli-o1ES3lJS4LlOhRP59NT8'  #Sample data\n",
        "\n",
        "download_file_from_google_drive(file_id, 'data/sampleData.csv')\n",
        "\n",
        "if not os.path.exists('newModels'):\n",
        "    os.mkdir('newModels')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRlH8WhKnJHn",
        "colab_type": "code",
        "outputId": "0bc1f3c0-9930-4bdc-efda-15209bfb5339",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 198
        }
      },
      "source": [
        "#Download sample data from google drive. \n",
        "\n",
        "if not os.path.exists('data'):\n",
        "    os.mkdir('data')\n",
        "\n",
        "#Google drive links looks like https://drive.google.com/open?id=1G-NhMv_AQli-o1ES3lJS4LlOhRP59NT8\n",
        "\n",
        "file_id = '1vZR7C45vQG4_SFHMJfmq9d2QK8Zgi8Dq'  #only put file ID from Gdrive link \n",
        "\n",
        "download_file_from_google_drive(file_id, 'data/sampleData.csv')\n",
        "\n",
        "#look at data in pandas\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "dataPandas = pd.read_csv('data/sampleData.csv')\n",
        "\n",
        "#This is how the data should be formatted, including all lower question column names. \n",
        "dataPandas.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Trying to get y friend to see a doctor but she...</td>\n",
              "      <td>This is most likely a Bartholin's Gland Cyst. ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Ease my worries pleaseMy husband is about to s...</td>\n",
              "      <td>Would be helpful with the exact values of the ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Severe shoulder, neck and back pain after teta...</td>\n",
              "      <td>I seriously don't think your pain derives from...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Knee problemHere's the run down.  This past we...</td>\n",
              "      <td>It's possible that you caused some damage whil...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Had a strange experience last night.Woke up in...</td>\n",
              "      <td>Sounds like you had an episode of positional h...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            question  \\\n",
              "0  Trying to get y friend to see a doctor but she...   \n",
              "1  Ease my worries pleaseMy husband is about to s...   \n",
              "2  Severe shoulder, neck and back pain after teta...   \n",
              "3  Knee problemHere's the run down.  This past we...   \n",
              "4  Had a strange experience last night.Woke up in...   \n",
              "\n",
              "                                              answer  \n",
              "0  This is most likely a Bartholin's Gland Cyst. ...  \n",
              "1  Would be helpful with the exact values of the ...  \n",
              "2  I seriously don't think your pain derives from...  \n",
              "3  It's possible that you caused some damage whil...  \n",
              "4  Sounds like you had an episode of positional h...  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZKYoyyDeIE5",
        "colab_type": "code",
        "outputId": "9e63d0b9-2c4c-4a85-e564-61ad302f4a34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "#Import relevant functions and objects\n",
        "\n",
        "from Scripts.dataset import create_dataset_for_bert\n",
        "from Scripts.models import MedicalQAModelwithBert\n",
        "from Scripts.loss import qa_pair_loss, qa_pair_cross_entropy_loss\n",
        "from Scripts.tokenization import FullTokenizer\n",
        "from Scripts.metrics import qa_pair_batch_accuracy\n",
        "\n",
        "#Set parameters and paths\n",
        "\n",
        "model_path='newModels' #to train brand new model, or use our pretrained model by uncommenting the line below\n",
        "# model_path='models/bertffn_crossentropy/bertffn/bertffn'\n",
        "\n",
        "data_path='data/' #This is where you put your own data!\n",
        "\n",
        "num_epochs=1\n",
        "num_gpu=1\n",
        "batch_size=64\n",
        "learning_rate=2e-5\n",
        "validation_split=0.2\n",
        "loss='categorical_crossentropy',\n",
        "pretrained_path='BioBertFolder/pubmed_pmc_470k/'\n",
        "max_seq_len=256  # Max is 512\n",
        "\n",
        "if loss == 'categorical_crossentropy':\n",
        "    loss_fn = qa_pair_cross_entropy_loss\n",
        "else:\n",
        "    loss_fn = qa_pair_loss\n",
        "    \n",
        "K.set_floatx('float32')\n",
        "\n",
        "tokenizer = FullTokenizer(os.path.join(pretrained_path, 'vocab.txt'))\n",
        "\n",
        "d = create_dataset_for_bert(\n",
        "    data_path, tokenizer=tokenizer, batch_size=batch_size,\n",
        "    shuffle_buffer=500000, dynamic_padding=True, max_seq_length=max_seq_len)\n",
        "\n",
        "eval_d = create_dataset_for_bert(\n",
        "    data_path, tokenizer=tokenizer, batch_size=batch_size,\n",
        "    mode='eval', dynamic_padding=True, max_seq_length=max_seq_len,\n",
        "    bucket_batch_sizes=[64, 64, 64])\n",
        "\n",
        "medical_qa_model = MedicalQAModelwithBert(\n",
        "    config_file=os.path.join(\n",
        "        pretrained_path, 'bert_config.json'),\n",
        "    checkpoint_file=os.path.join(pretrained_path, 'biobert_model.ckpt'))\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(lr=learning_rate)\n",
        "\n",
        "medical_qa_model.compile(\n",
        "    optimizer=optimizer, loss=loss_fn, metrics=[qa_pair_batch_accuracy])\n",
        "\n",
        "epochs = num_epochs\n",
        "\n",
        "loss_metric = tf.keras.metrics.Mean()\n",
        "\n",
        "medical_qa_model.fit(d, epochs=epochs)\n",
        "medical_qa_model.summary()\n",
        "medical_qa_model.save_weights(model_path)\n",
        "medical_qa_model.evaluate(eval_d)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Failed to load GPU Faiss: No module named 'faiss.swigfaiss_gpu'\n",
            "Faiss falling back to CPU-only.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "TF Record not found\n",
            "Converting file data/sampleData.csv to TF Record\n",
            "   3329/Unknown - 3156s 948ms/step - loss: 1.8336 - qa_pair_batch_accuracy: 0.4226"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}